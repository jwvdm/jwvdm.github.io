<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link href="https://fast.fonts.com/cssapi/cac43e8c-6965-44df-b8ca-9784607a3b53.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="/assets/css/jwvdm.css" rel="stylesheet">
    <link rel="shortcut icon" type="image/x-icon" href="/assets/ico/favicon.ico" >

    <title>Jan-Willem van de Meent</title>


    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="/assets/js/html5shiv.js"></script>
      <script src="/assets/js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="container">

      <div class="header row">
        <div class="col">
  <div class="row">
  
    <div class="title col-sm-6 col-md-8">
      JAN-WILLEM VAN DE MEENT
    </div>
  
    <div class="col-sm-6 col-md-4">
      <ul class="nav nav-pills float-right">
        
        
        <li class="active">
          <a href="/" title="">Home</a>
        </li>
        
        
        <li >
          <a href="/talks/" title="">Talks</a>
        </li>
        
        
        <li >
          <a href="/publications/" title="">Publications</a>
        </li>
        
      </ul>
    </div>
  </div>
</div>

       
      </div>

      <div class="content row">
        <div class="col">
        <div id="info" class="row">
  <div class="col-12 col-sm-6 col-md-8">
     <div id="bio">
      
      <p>I am an Associate Professor (Universitair Hoofddocent) at the University of Amsterdam, where I co-direct the <a href="https://amlab.science.uva.nl/">AMLab</a> with Max Welling. I am also an Assistant Professor (on leave) at Northeastern University, where I continue to advise and collaborate.</p>

<p>My research develops models for artificial intelligence by combining probabilistic programming and deep learning. Our work seeks to understand what inductive biases can enable models to generalize from limited data. These inductive biases can take the form of a simulator that incorporates knowledge of an underlying physical system, causal structure, or symmetries of the underlying domain. We combine model development with research on methods for inference in these models. We also put this work into practice in collaborations with researchers in robotics, NLP, healthcare, and the physical sciences.</p>

<p>The technical backbone in much of our work is probabilistic programming. I am one of the creators of <a href="https://probprog.github.io/anglican/index.html">Anglican</a>, a probabilistic language based on Clojure. My group currently develops <a href="https://github.com/probtorch/probtorch">Probabilistic Torch</a>, a library for deep generative models that extends PyTorch. I am writing a book on probabilistic programming, a draft of which is available on <a href="https://arxiv.org/abs/1809.10756">arXiv</a>. I am also a co-chair of the international conference on probabilistic programming (<a href="https://probprog.cc/">PROBPROG</a>).</p>

     </div>

    <div id="news">
      <h1>News</h1>
      
      <p><strong>SEP 2021 ∙</strong> As of 1 September, I will start at the University of Amsterdam and will be on leave from Northeastern University. I will continue to advise current students, but am not recruiting PhD students or postdocs at Northeastern this cycle. I will participate in PhD recruiting through <a href="https://ellis.eu/news/ellis-phd-program-call-for-applications-deadline-november-15-2021">ELLIS</a>, as will other faculty at AMLab. Please contact me at my UvA address with inquiries.</p>

<p><strong>JUL 2021 ∙</strong> I am delighted to have received the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2047253">NSF CAREER award</a> award for my research on deep learning and probabilistic programming!</p>

<p><strong>JUL 2021 ∙</strong> <a href="http://mathserver.neu.edu/~robin/">Robin Walters</a> has received his first grant as PI for his work on representation-theoretic foundations of deep learning  under the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2134178">NSF the Scale MoDL program</a>!</p>

<p><strong>JUN 2021 ∙</strong> The NSF has <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1901117">funded</a> joint work on MDP abstractions with <a href="https://www.ccs.neu.edu/home/lsw/">Lawson Wong</a>, <a href="https://www.ccs.neu.edu/home/rplatt/">Rob Platt</a>, and <a href="http://mathserver.neu.edu/robin/">Robin Walters</a>!</p>

<p><strong>MAY 2021 ∙</strong> <a href="https://www.khoury.northeastern.edu/people/samuel-stites/">Sam</a> and <a href="http://www.ccs.neu.edu/home/hzimmermann/">Heiko’s</a> paper on inference combinators will appear at UAI 2021!</p>

<p><strong>MAY 2021 ∙</strong> <a href="https://www.khoury.northeastern.edu/people/hao-wu/">Hao’s</a> and 
<a href="https://babak0032.github.io/">Babak’s</a> paper on Conjugate Energy-based Models was will appear at ICML 2021!</p>

<p><strong>JAN 2021 ∙</strong> <a href="https://alicanb.github.io/">Alican’s</a> and <a href="https://babak0032.github.io/">Babak’s</a> paper on rate-regularization and generalization in VAEs will appear at AISTATS 2021.</p>

<p><strong>JAN 2021 ∙</strong> Our group will be presenting 3 extended abstracts at <a href="http://approximateinference.org/">AABI</a> this year [<a href="https://openreview.net/forum?id=tvxL1eqPl9Y">1</a>, <a href="https://openreview.net/forum?id=oU7dRDX8SNA">2</a>, <a href="https://openreview.net/forum?id=4k58RmAD02">3</a>].</p>

<p><strong>DEC 2020 ∙</strong> <a href="https://sites.google.com/view/obiza">Ondrej’s</a> paper on action priors will appear at AAMAS 2021.</p>

<!-- **OCT 2020 ∙** I co-chaired the second edition of the International Conference on Probabilistic Programming ([PROBPROG 2020](https://probprog.cc)) -->

<!-- **SEP 2020 ∙** [Eli's](https://esennesh.github.io/) paper with [Zulqarnain](https://scholar.google.com/citations?user=rYq6RQ4AAAAJ) on inferring participant and stimuli embeddings from neuroimaging data will appear at NeurIPS 2020 [[ArXiv](https://arxiv.org/abs/1906.08901)] -->

<!-- **JUN 2020 ∙** [Jered's](https://www.khoury.northeastern.edu/people/denis-jered-mcinerney/) paper on Query-Focused EHR summarization will appear at MLHC 2020 [[ArXiv](https://arxiv.org/abs/2004.04645)] -->

<!-- **MAY 2020 ∙** [Hao's](https://www.khoury.northeastern.edu/people/hao-wu/) paper on Amortized Population Gibbs Samplers will appear at ICML 2020 [[ArXiv](https://arxiv.org/abs/1911.01382)] -->

<!-- **NOV 2019 ∙** New working paper by [Alican Bozkurt](https://alicanb.github.io) and [Babak Esmaeili](https://www.khoury.northeastern.edu/people/babak-esmaeili/) on evaluating combinatorial generlization in VAEs [[Arxiv](https://arxiv.org/abs/1911.04594)]
 -->

<!-- **AUG 2019 ∙** DARPA has funded joint work with [Charles River Analytics](https://www.cra.com/), UBC, and UC Irvine under the program [Learning with Less Labels (LwLL)](https://www.darpa.mil/program/learning-with-less-labels)
 -->
<!-- **JUN 2019 ∙** The NSF has funded joint work with [Byron Wallace](http://www.byronwallace.com/) on disentangled representations for text! [Award #1901117](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1901117&HistoricalAwards=false)
 -->
<!-- **DEC 2018 ∙** Two papers by [Babak](https://www.ccis.northeastern.edu/people/babak-esmaeili/) will appear at [AISTATS 2019](https://www.aistats.org/):  *1. Structured Disentangled Representations* [[arXiv](https://arxiv.org/abs/1804.02086)] (with [Hao Wu](https://www.ccis.northeastern.edu/people/hao-wu/), [Sarthak Jain](https://www.ccis.northeastern.edu/people/hao-wu/), and [Alican Bozkurt](https://alicanb.github.io/)) *2. Structured Neural Topic Models for Reviews* [[arXiv](https://arxiv.org/abs/1812.05035)]
 -->
<!-- **OCT 2018 ∙** A draft of our book *An Introduction to Probabilistic Programming* is now publicly available [[arXiv](https://arxiv.org/abs/1809.10756)]. This book is intended as a graduate-level introduction to probabilistic programming languages and methods for inference in probabilistic programs
 -->
<!-- **AUG 2018 ∙** The NSF has funded our work on deep probabilistic models for individual variation in neuroimaging experiment! [Award #1835309](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1835309), co-investigators [Ajay Satpute](https://cos.northeastern.edu/people/ajay-satpute/), [Benjamin Hutchinson](https://psychology.uoregon.edu/profile/bhutch/), [Jennifer Dy](https://www.ece.neu.edu/fac-ece/jdy/), and [Sarah Ostaddabas](https://www.ece.neu.edu/people/ostadabbas-sarah).
 -->
<!-- **AUG 2018 ∙** Congrats to [Sarthak Jain](https://www.ccis.northeastern.edu/people/sarthak-jain-2/) on the acceptance of his paper *Learning Disentangled Representations of Texts with Application to Biomedical Abstracts* at EMNLP
[[arXiv](https://arxiv.org/pdf/1804.07212.pdf)].
 -->

<!-- **DEC 2017 ∙** We have open-sourced *Probabilistic Torch* [[github](https://github.com/probtorch/probtorch)], a library for deep generative models that extends [PyTorch](http://pytorch.org). This release accompanies our paper at *NIPS* [[paper](http://papers.nips.cc/paper/by-source-2017-3021)].
 -->

<!-- **DEC 2017 ∙** Our extended abstract *"Inference Trees: Adaptive Inference with Exploration"* was accepted at the *NIPS workshop on Advances in Approximate Bayesian Inference* [[website](http://approximateinference.org/accepted/)]. -->

<!-- **NOV 2017 ∙** I am teaching *CS 7140 – Advanced Machine Learning* this spring [[website](https://course.ccs.neu.edu/cs7140sp18)].
 -->

<!-- **NOV 2017 ∙** I will speak at the *AAAI 2018 Workshop on Planning and Inference* in Februari [[website](http://www.cs.tufts.edu/~roni/PI2018/index.html)]
 -->

<!-- **SEP 2017 ∙** Our paper *"Learning Disentangled Representations with Semi-Supervised Deep Generative Models"* has been accepted for publication at *NIPS* [[arxiv](https://arxiv.org/abs/1706.00400)].
 -->
<!-- **FEB 2017 ∙** David's paper *"Design and Implementation of Probabilistic Programming Language Anglican"* has appeared in the proceedings of [IFL](https://dtai.cs.kuleuven.be/events/ifl2016/). 
 -->
<!-- **AUG 2016 ∙** Our paper *"Bayesian Optimization for Probabilistic Programs"* was accepted at [NIPS](https://nips.cc/Conferences/2016/AcceptedPapers). 

**JUL 2016 ∙** I have lectured in the [PPAML summer school](http://ppaml.galois.com/wiki/wiki/SummerSchools/2016/LectureMaterials) on Probabilistic Programming. Materials can be found [here](talks/#tutorials).

**JUL 2016 ∙** Anglican 1.0.0 is now up on [Clojars](https://clojars.org/anglican/versions/1.0.0).

**JUL 2016 ∙** I will be teaching the [CS 6220](http://www.ccs.neu.edu/course/cs6220f16/sec3/) course on *Data Mining Techniques*. 

**JUN 2016 ∙** Our paper on "*Interacting Particle Markov Chain Monte Carlo*"" appeared at [ICML](http://proceedings.mlr.press/v48/rainforth16.html).

**MAY 2016 ∙** Our paper on "*Black Box Policy Search with Probabilistic Programs*" appeared at [AISTATS](http://proceedings.mlr.press/v51/vandemeent16.html).
 -->
<!-- ### DEC 2015

Thanks to all participants for a great NIPS workshop on [Black Box Inference and Learning](http://www.blackboxworkshop.org). Accepted abstracts can be found [here](http://www.blackboxworkshop.org/accepted-papers/). -->

    </div>
 
  </div>

  <div class="col-12 col-sm-6 col-md-4">
    <div id="contact" class="card">
      <div class="card-body"> 
        <img alt="contact-photo" src="/assets/images/jwvdm_headshot_small.jpg">
        
        <p><strong>Jan-Willem van de Meent</strong></p>

<p>Associate Professor  (UHD) <br />
University of Amsterdam <br />
Informatics Institute <br />
Science Park 904, Room C3.259 <br />
Postbus 94323, 1090 GH Amsterdam <br />
<a href="&#109;ai&#108;to&#58;j&#123;chie6Aet&#126;w&#123;chie6Aet&#126;vandemeent&#123;varaiRi8&#126;uva&#123;chie6Aet&#126;nl " onmouseover="this.href=this.href.replace(/&#123;varaiRi8&#126;/,'&#64;').replace(/&#123;chie6Aet&#126;/g,'&#46;').replace(/&#123;Oe8eed6M&#126;/g,'&#95;').replace(/&#123;oaPoo7eo&#126;/g,'&#45;')"><span class="o3m41l" data-x="j.w.vandemeent" data-y="uva.nl "></span></a></p>

<p>Assistant Professor (on leave) <br />
Northeastern University<br />
Khoury College of Computer Sciences <br />
<a href="&#109;ai&#108;to&#58;j&#123;chie6Aet&#126;vandemeent&#123;varaiRi8&#126;northeastern&#123;chie6Aet&#126;edu " onmouseover="this.href=this.href.replace(/&#123;varaiRi8&#126;/,'&#64;').replace(/&#123;chie6Aet&#126;/g,'&#46;').replace(/&#123;Oe8eed6M&#126;/g,'&#95;').replace(/&#123;oaPoo7eo&#126;/g,'&#45;')"><span class="o3m41l" data-x="j.vandemeent" data-y="northeastern.edu "></span></a></p>



        <a href="https://scholar.google.com/citations?user=aCGsfUAAAAAJ" alt="Google scholar">
          <img class="contact-icon" src="/assets/images/google_scholar.svg"></img></a>

        <a rel="me" href="https://mastodon.online/@jwvdm" alt="Mastodon"><img class="contact-icon" src="/assets/images/mastodon.svg"></img></a>

        <a href="https://twitter.com/jwvdm" alt="Twitter">
          <img class="contact-icon" src="/assets/images/twitter.svg"></img></a>

        <a href="https://github.com/jwvdm" alt="Github">
          <img class="contact-icon" src="/assets/images/github.svg"></img></a>
        <a href="https://bitbucket.org/jwvdm/" alt="Bitbucket">
          <img class="contact-icon" src="/assets/images/bitbucket.svg"></img></a>
      </div>
    </div>
  </div>
</div>      

<div id="group">
  <h1>
    Current Students and Postdocs
</h1>

<div class="row">
  <div class="col">
    <img class="headshot" alt="Robin Walters" src="/assets/images/robin.jpg"><br>
    <a href="http://mathserver.neu.edu/robin/">
        Robin Walters</a><br>
    Postdoctoral Fellow<br>
  </div>
  <div class="col">
    <img class="headshot" alt="Ondrej Biza" src="/assets/images/ondrej.jpg"><br>
    <a href="https://sites.google.com/view/obiza">
        Ondrej Biza</a><br>
    Ph.D. Candidate<br>
    <i>Co-advised with <br> 
     <a href="https://www.ccs.neu.edu/home/rplatt/">Robert Platt</a> and <br>
     <a href="https://www.ccs.neu.edu/home/lsw/">Lawson Wong</a></i>
  </div>
  <div class="col">
    <img class="headshot" alt="Babak Esmaeili" src="/assets/images/babak.jpg"><br>
    <a href="https://babak0032.github.io/">
        Babak Esmaeili</a><br>
    Ph.D. Candidate
  </div>
  <div class="col">
    <img class="headshot" alt="Jered McInerney" src="/assets/images/jered.jpg"><br>
    <a href="https://www.khoury.northeastern.edu/people/denis-jered-mcinerney/">
        Jered McInerney</a><br>
    Ph.D. Candidate<br>
    <i>Co-advised with <br> <a href="http://www.byronwallace.com/">Byron Wallace</a></i>
  </div>
<!--   <div class="col">
    <img class="headshot" alt="Iris Seaman" src="/assets/images/iris.jpg"><br>
    <a href="https://www.khoury.northeastern.edu/people/iris-rubi-seaman/">
        Iris Seaman</a><br>
    Ph.D. Candidate    
  </div>
 -->  
   <div class="col">
    <img class="headshot" alt="Eli Sennesh" src="/assets/images/eli.jpg"><br>
    <a href="https://www.khoury.northeastern.edu/people/eli-sennesh/">
        Eli Sennesh</a><br>
    Ph.D. Candidate<br>
    <i>Co-advised with <br> <a href="https://lisafeldmanbarrett.com/">Lisa Feldman Barrett</a> and <br>
     <a href="https://cos.northeastern.edu/people/karen-quigley/">Karen Quigley</a> </i>
  </div>
  <div class="col">
    <img class="headshot" alt="Niklas Smedemark-Margulies" src="/assets/images/niklas.jpg"><br>
    <a href="https://www.khoury.northeastern.edu/people/niklas-smedemark-margulies/">
        Niklas Smedemark-Margulies</a><br>
    Ph.D. Candidate<br>
  </div>
  <div class="col">
    <img class="headshot" alt="Sam Stites" src="/assets/images/sam.jpg"><br>
    <a href="https://stites.io/">
        Sam Stites</a><br>
    Ph.D. Candidate<br>
  </div>
  <div class="col">
    <img class="headshot" alt="Hao Wu" src="/assets/images/hao.jpg"><br>
    <a href="https://hao-w.github.io/">
        Hao Wu</a><br>
    Ph.D. Candidate   
  </div>
  <div class="col">
    <img class="headshot" alt="Xiongyi Zhang" src="/assets/images/xiongyi.jpg"><br>
    <a href="https://www.khoury.northeastern.edu/role/phd-students/">
        Xiongyi Zhang</a><br>
    Ph.D. Candidate<br>
    <i>Co-advised with <br> <a href="http://www.byronwallace.com/">Byron Wallace</a></i>
  </div>
  <div class="col">
    <img class="headshot" alt="Heiko Zimmermann" src="/assets/images/heiko.jpg"><br>
    <a href="http://www.ccs.neu.edu/home/hzimmermann/">
        Heiko Zimmermann</a><br>
    Ph.D. Candidate
  </div>
</div>
   
    


</div>

<div id="papers">
  <h1>
  Working Papers
</h1>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="vandemeent_ftml_2018" src="/assets/images/vandemeent_ftml_2018_3.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      An Introduction to Probabilistic Programming
    </div>
    <div class="pub-venue">
      <a href="https://arxiv.org/abs/1809.10756">ArXiv Pre-print</a>
    </div>
    <div class="pub-auth">
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="http://www.robots.ox.ac.uk/~brooks/">Brooks Paige</a>,
      <a href="https://cs.kaist.ac.kr/people/view?idx=552&kind=faculty&menu=167">Hongseok Yang</a>,
      <a href="http://www.robots.ox.ac.uk/~fwood/">Frank Wood</a>.
    </div>
    <div class="pub-abst">
          This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages.
   </div>
    <div class="pub-links">
      [<a href="https://arxiv.org/abs/1809.10756">arXiv</a>]
    </div>
  </div>
</div>





<!--
<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="bozkurt_cract_2018" src="/assets/images/bozkurt_cract_2018.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Can VAEs Generate Novel Examples?
    </div>
    <div class="pub-venue">
      <a href="https://ml-critique-correct.github.io/">NeurIPS 2018 Workshop on Critiquing and Correcting Trends in Machine Learning</a>
    </div>
    <div class="pub-auth">
      <a href="http://alicanb.github.io">Alican Bozkurt</a>,
      <a href="https://www.khoury.northeastern.edu/people/babak-esmaeili/">Babak Esmaeili</a>,
      <a href="http://www.ece.neu.edu/people/brooks-dana">Dana H. Brooks</a>,
      <a href="http://www.ece.neu.edu/fac-ece/jdy/">Jennifer Dy</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
    We investigate to what extent widely employed variational autoencoder (VAE) architectures can generate examples that were not previously seen in the training data. We consider both generalization to new examples of previously seen
    classes, and generalization to the classes that were withheld from the training set. In both cases, we find that reconstructions are closely approximated by nearest neighbors for higher-dimensional parameterizations. When generalizing to unseen classes however, lower-dimensional parameterizations offer a clear advantage
    </div>
    <div class="pub-links">
      [<a href="https://alicanb.github.io/publication/cract-vae/">Github</a>] |
      [<a href="https://ml-critique-correct.github.io/">NeurIPS CRACT Workshop</a>]
    </div>
  </div>
</div>
 -->


<h1>
  Selected Papers
</h1>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="sennesh_biopsych_2021" src="/assets/images/sennesh_biopsych_2021.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Interoception as modeling, allostasis as control
    </div>
    <div class="pub-venue">
      <a href="https://www.sciencedirect.com/science/article/pii/S0301051121002350">Biological Psychology, 2021</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/eli-sennesh/">Eli Sennesh*</a>,
      <a href="https://cos.northeastern.edu/people/jordan-theriault/">Jordan Theriault</a>,
      <a href="https://coe.northeastern.edu/people/brooks-dana/">Dana Brooks</a>,
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="https://cos.northeastern.edu/people/lisa-barrett/">Lisa Feldman Barrett</a>,
      <a href="https://cos.northeastern.edu/people/karen-quigley/">Karen Quigley</a>,
    </div>
    <div class="pub-abst">
      The brain regulates the body by anticipating its needs and attempting to meet them before they arise – a process called allostasis. Allostasis requires a model of the changing sensory conditions within the body, a process called interoception. In this paper, we examine how interoception may provide performance feedback for allostasis. We suggest studying allostasis in terms of control theory, reviewing control theory’s applications to related issues in physiology, motor control, and decision making. We synthesize these by relating them to the important properties of allostatic regulation as a control problem. We then sketch a novel formalism for how the brain might perform allostatic control of the viscera by analogy to skeletomotor control, including a mathematical view on how interoception acts as performance feedback for allostasis. Finally, we suggest ways to test implications of our hypotheses.
    </div>
    <div class="pub-links">
      [<a href="https://www.sciencedirect.com/science/article/pii/S0301051121002350">Biological Psychology, 2021</a>]  | [<a href="https://psyarxiv.com/2ymuj">PsyArXiv</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="zimmermann_aabi_2021" src="/assets/images/zimmermann_aabi_2021.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Nested Variational Inference
    </div>
    <div class="pub-venue">
      <a href="https://proceedings.neurips.cc/paper/2021/hash/ab49b208848abe14418090d95df0d590-Abstract.html">
        Neural Information Processing Systems (NeurIPS), 2021
      </a>
    </div>
    <div class="pub-auth">
      <a href="http://heikozimmermann.com/">Heiko Zimmermann</a>,
      <a href="https://www.khoury.northeastern.edu/people/hao-wu/">Hao Wu</a>,
      <a href="https://www.khoury.northeastern.edu/people/babak-esmaeili/">Babak Esmaeili</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
        We develop nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. Our experiments apply NVI to (a) sample from a multimodal distribution using a learned annealing path (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model and (c) to perform amortized inference in hierarchical deep generative models. We observe that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size.
   </div>
    <div class="pub-links">
      [<a href="https://proceedings.neurips.cc/paper/2021/file/ab49b208848abe14418090d95df0d590-Paper.pdf">NeurIPS 2021</a>] 
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="zhang_arxiv_2021" src="/assets/images/zhang_arxiv_2021.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Disentangling Representations of Text by Masking Transformers
    </div>
    <div class="pub-venue">
      <a href="https://aclanthology.org/2021.emnlp-main.60/">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.linkedin.com/in/xiongyi-zhang">Xiongyi Zhang</a>,
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="http://www.byronwallace.com">Byron C. Wallace</a>.
    </div>
    <div class="pub-abst">
      Representations in large language models such as BERT encode a range of features into a single vector. In this work, we explore whether it is possible to learn disentangled representations by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews.
   </div>
    <div class="pub-links">
      [<a href="https://aclanthology.org/2021.emnlp-main.60.pdf">EMNLP</a> <a href="https://arxiv.org/abs/2104.07155">arXiv</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="wu_aabi_2021" src="/assets/images/wu_aabi_2021.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Conjugate Energy-Based Models
    </div>
    <div class="pub-venue">
      <a href="https://icml.cc/">International Conference on Machine Learning (ICML), 2021</a><p>
      <a href="https://openreview.net/group?id=approximateinference.org/AABI/2021/Symposium">3rd Symposium on Advances in Approximate Bayesian Inference (AABI 2021)</a>
    </div>
    <div class="pub-auth">
      <a href="https://hao-w.github.io/">Hao Wu*</a>,
      <a href="https://www.khoury.northeastern.edu/people/babak-esmaeili/">Babak Esmaeili*</a>,
      <a href="https://labs.oracle.com/pls/apex/f?p=LABS:bio:0:2069">Michael Wick</a>,
      <a href="https://jtristan.github.io/">Jean-Baptiste Tristan</a>,
      <a href="/">Jan-Willem van de Meent</a>,
    </div>
    <div class="pub-abst">
    We propose conjugate energy-based models (CEBMs), a class of deep latent-variable models with a tractable posterior. Conjugate EBMs have similar use cases as variational autoencoders, in the sense that they learn an unsupervised mapping between data and latent variables. However these models omit a generator, which allows them to learn more flexible notions of similarity between data points. Our experiments demonstrate that conjugate EBMs achieve competitive results in terms of image modelling, predictive power of latent space, and out-of-distribution detection on a variety of datasets.
   </div>
    <div class="pub-links">
      [<a href="http://proceedings.mlr.press/v139/wu21a/wu21a.pdf">ICML 2021</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="stites_combinators_2021" src="/assets/images/stites_combinators_2021.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Learning Proposals for Probabilistic Programs with Inference Combinators
    </div>
    <div class="pub-venue">
      <a href="https://www.auai.org/uai2021/">Uncertainty in Artificial Intelligence (UAI), 2021</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/samuel-stites/">Sam Stites*</a>,
      <a href="http://heikozimmermann.com/">Heiko Zimmermann*</a>,
      <a href="https://www.khoury.northeastern.edu/people/hao-wu/">Hao Wu</a>,
      <a href="https://www.khoury.northeastern.edu/people/eli-sennesh/">Eli Sennesh</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
    We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernels and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework in applications to advanced variational methods based on amortized Gibbs sampling and annealing.
   </div>
    <div class="pub-links">
      [<a href="https://proceedings.mlr.press/v161/stites21a.html">UAI 2021</a>]
      [<a href="https://arxiv.org/abs/2103.00668">arXiv</a>]
      [<a href="https://github.com/probtorch/combinators/">Code</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="biza_aamas_2021" src="/assets/images/biza_aamas_2021.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Action Priors for Large Action Spaces in Robotics
    </div>
    <div class="pub-venue">
      <a href="http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p205.pdf">20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2021</a>
    </div>
    <div class="pub-auth">
      <a href="https://sites.google.com/view/obiza/">Ondrej Biza</a>,
      <a href="https://sites.google.com/view/obiza/">Dian Wang</a>,
      <a href="https://www.ccs.neu.edu/home/rplatt/">Robert Platt</a>.
      <a href="/">Jan-Willem van de Meent</a>.
      <a href="https://www.ccs.neu.edu/home/lsw/">Lawson Wong</a>,
    </div>
    <div class="pub-abst">
      In robotics, it is often not possible to learn useful policies using pure model-free reinforcement learning without significant reward shaping or curriculum learning. As a consequence, many researchers rely on expert demonstrations to guide learning. However, acquiring expert demonstrations can be expensive. This paper proposes an alternative approach where the solutions of previously solved tasks are used to produce an action prior that can facilitate exploration in future tasks. The action prior is a probability distribution over actions that summarizes the set of policies found solving previous tasks. Our results indicate that this approach can be used to solve robotic manipulation problems that would otherwise be infeasible without expert demonstrations.
    </div>
    <div class="pub-links">
      [ <a href="http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p205.pdf">AAMAS 2021</a>] | [<a href="https://arxiv.org/abs/2101.04178">arXiv</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="bozkurt_arxiv_2019" src="/assets/images/bozkurt_arxiv_2019.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Rate-regularization and Generalization in VAEs
    </div>
    <div class="pub-venue">
      <a href="https://proceedings.mlr.press/v130/bozkurt21a.html">Artificial Intelligence and Statistics (AISTATS), 2021</a>
    </div>
    <div class="pub-auth">
      <a href="https://alicanb.github.io">Alican Bozkurt*</a>,
      <a href="https://www.khoury.northeastern.edu/people/babak-esmaeili/">Babak Esmaeili*</a>,
      <a href="https://jtristan.github.io">Jean-Baptiste Tristan</a>,
      <a href="http://www.ece.neu.edu/people/brooks-dana">Dana H. Brooks</a>,
      <a href="http://www.ece.neu.edu/fac-ece/jdy/">Jennifer Dy</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
      Variational autoencoders (VAEs) optimize an objective that comprises a reconstruction loss (the distortion) and a KL term (the rate). The rate is an upper bound on the mutual information, which is often interpreted as a regularizer that controls the degree of compression. We here examine whether inclusion of the rate term also improves generalization. We perform rate-distortion analyses in which we control the strength of the rate term, the network capacity, and the difficulty of the generalization problem. Lowering the strength of the rate term paradoxically improves generalization in most settings, and reducing the mutual information typically leads to underfitting. Our results suggest that the standard spherical Gaussian prior is not an inductive bias that typically improves generalization, prompting further work to understand what choices of priors improve generalization in VAEs.
    </div>
    <div class="pub-links">
      [<a href="https://proceedings.mlr.press/v130/bozkurt21a.html">AISTATS 2021</a>] | [<a href="https://arxiv.org/abs/1911.04594">arXiv</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="biza_arxiv_2020" src="/assets/images/biza_arxiv_2020.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Learning discrete state abstractions with deep variational inference
    </div>
    <div class="pub-venue">
      <a href="https://openreview.net/group?id=approximateinference.org/AABI/2021/Symposium">3rd Symposium on Advances in Approximate Bayesian Inference (AABI), 2021</a>
    </div>
    <div class="pub-auth">
      <a href="https://sites.google.com/view/obiza/">Ondrej Biza</a>,
      <a href="https://www.ccs.neu.edu/home/rplatt/">Robert Platt</a>.
      <a href="/">Jan-Willem van de Meent</a>.
      <a href="https://www.ccs.neu.edu/home/lsw/">Lawson Wong</a>,
    </div>
    <div class="pub-abst">
      Abstraction is crucial for effective sequential decision making in domains with large state spaces. In this work, we propose a variational information bottleneck method for learning approximate bisimulations, a type of state abstraction. Our method is suited for environments with high-dimensional states and learns from a stream of experience collected by an agent acting in a Markov decision process. Through a learned discrete abstract model, we can efficiently plan for unseen goals in a multi-goal Reinforcement Learning setting. We test our method in simplified robotic manipulation domains with image states. We also compare it against previous model-based approaches to finding bisimulations in discrete grid-world-like environments.
    </div>
    <div class="pub-links">
      [<a href="https://openreview.net/group?id=approximateinference.org/AABI/2021/Symposium ">AABI 2021</a>] | [<a href="https://arxiv.org/abs/2003.04300">arXiv</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="sennesh_arxiv_2019" src="/assets/images/sennesh_arxiv_2019.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Neural Topographic Factor Analysis for fMRI Data
    </div>
    <div class="pub-venue">
      <a href="https://proceedings.neurips.cc/paper/2020/hash/8c3c27ac7d298331a1bdfd0a5e8703d3-Abstract.html">Neural Information Processing Systems (NeurIPS), 2020</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/eli-sennesh/">Eli Sennesh*</a>,
      <a href="https://www.linkedin.com/in/zulqarnain-khan-3582384a">Zulqarnain Khan*</a>,
      <a href="https://www.ece.neu.edu/fac-ece/jdy/">Jennifer Dy</a>,
      <a href="https://cos.northeastern.edu/people/ajay-satpute/">Ajay Satpute</a>,
      <a href="https://psychology.uoregon.edu/profile/bhutch/">Benjamin Hutchinson</a>,
      <a href="/">Jan-Willem van de Meent</a>,
    </div>
    <div class="pub-abst">
      Neuroimaging experiments produce a large volume (gigabytes) of high-dimensional spatio-temporal data for a small number of sampled participants and stimuli. To enable the analysis of variation fMRI experiments, we propose Neural Topographic Factor Analysis (NTFA), a deep generative model that parameterizes factors as functions of embeddings for participants and stimuli.
    </div>
    <div class="pub-links">
      [<a href="https://proceedings.neurips.cc/paper/2020/hash/8c3c27ac7d298331a1bdfd0a5e8703d3-Abstract.html">NeurIPS 2020</a>]  | [<a href="https://arxiv.org/abs/1906.08901">arXiv</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="wu_arxiv_2019" src="/assets/images/wu_arxiv_2019.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Amortized Population Gibbs Samplers with Neural Sufficient Statistics
    </div>
    <div class="pub-venue">
      <a href="http://proceedings.mlr.press/v119/wu20h.html">International Conference on Machine Learning (ICML), 2020</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/hao-wu/">Hao Wu</a>,
      <a href="http://heikozimmermann.com/">Heiko Zimmermann</a>,
      <a href="https://www.khoury.northeastern.edu/people/eli-sennesh/">Eli Sennesh</a>,
      <a href="https://www.tuananhle.co.uk/">Tuan Anh Le</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
    We develop amortized population Gibbs (APG) samplers, a new class of autoencoding variational methods for deep probabilistic models. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train block proposals to approximate Gibbs conditionals by minimizing an inclusive KL divergence. To ensure that proposals generalize across input datasets that vary in size, we introduce a new parameterization in terms of neural sufficient statistics. Experiments demonstrate that learned proposals converge to the known analytical conditional posterior in conjugate models, and that APG samplers can learn inference networks for highly-structured deep generative models when the conditional posteriors are intractable.
   </div>
    <div class="pub-links">
      [<a href="http://proceedings.mlr.press/v119/wu20h.html">ICML 2020</a>] | [<a href="https://arxiv.org/abs/1911.01382">arXiv</a>] | [<a href="https://icml.cc/virtual/2020/poster/6715">Video</a>] | [<a href="https://github.com/hao-w/apg-samplers">Code</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="mcinerney_arxiv_2020" src="/assets/images/mcinerney_arxiv_2020.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Query-Focused EHR Summarization to Aid Imaging Diagnosis
    </div>
    <div class="pub-venue">
      <a href="https://www.mlforhc.org/accepted-papers">Machine Learning for Healthcare (MLHC), 2020</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/denis-jered-mcinerney/">Jered McInerney</a>,
      <a href="https://scholar.google.com/citations?user=-AmG1czkX94C&hl=en">Borna Dabiri</a>,
      <a href="https://connects.catalyst.harvard.edu/Profiles/display/Person/172272">Anne-Sophie Touret</a>,
      <a href="https://researchfaculty.brighamandwomens.org/BRIProfile.aspx?id=366">Geoffrey Young</a>
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="http://www.byronwallace.com">Byron C. Wallace</a>.
    </div>
    <div class="pub-abst">
      Electronic Health Records (EHRs) provide vital contextual information to radiologists and other physicians when making a diagnosis. Unfortunately, because a given patient's record may contain hundreds of notes and reports, identifying relevant information within these in the short time typically allotted to a case is very difficult. We propose and evaluate Tranformer-based models that extract text snippets from patient records to aid diagnosis. We train these models by using groups of International Classification of Diseases (ICD) codes observed in 'future' records serve as noisy proxies for 'downstream' diagnoses. Evaluationsby radiologists demonstrate that these distantly supervised models yield better extractive summaries than do unsupervised approaches.
    </div>
    <div class="pub-links">
      [<a href="https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5f22cc34917e1c6f49e9116b/1596116025493/115_CameraReadySubmission_Query_Focused_EHR_Summarization_to_Aid_Imaging_Diagnosis.pdf">MLHC 2020</a>] |
      [<a href="https://arxiv.org/abs/2004.04645">arXiv</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="esmaeili_arxiv_2018" src="/assets/images/esmaeili_arxiv_2018.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Structured Disentangled Representations
    </div>
    <div class="pub-venue">
      <a href="http://proceedings.mlr.press/v89/esmaeili19a.html">The 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), 2019</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/babak-esmaeili/">Babak Esmaeili</a>,
      <a href="https://www.khoury.northeastern.edu/people/hao-wu/">Hao Wu</a>,
      <a href="https://www.khoury.northeastern.edu/people/sarthak-jain-2/">Sarthak Jain</a>,
      <a href="http://alicanb.github.io">Alican Bozkurt</a>,
      <a href="http://www.robots.ox.ac.uk/~nsid/">N. Siddharth</a>,
      <a href="http://www.robots.ox.ac.uk/~brooks/">Brooks Paige</a>,
      <a href="http://www.ece.neu.edu/people/brooks-dana">Dana H. Brooks</a>,
      <a href="http://www.ece.neu.edu/fac-ece/jdy/">Jennifer Dy</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
      Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks.
<!--       We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.
 -->    </div>
    <div class="pub-links">
      [<a href="http://proceedings.mlr.press/v89/esmaeili19a/esmaeili19a.pdf">PDF</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="esmaeili_arxiv_2018b" src="/assets/images/esmaeili_arxiv_2018b.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Structured Neural Topic Models for Reviews
    </div>
    <div class="pub-venue">
      <a href="http://proceedings.mlr.press/v89/esmaeili19b.html">The 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), 2019</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/babak-esmaeili/">Babak Esmaeili</a>,
      Hongyi Huang,
      <a href="http://www.byronwallace.com">Byron C. Wallace</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
      We present Variational Aspect-based Latent Topic Allocation (VALTA), a family of autoencoding topic models that learn aspect-based representations of reviews. VALTA defines a user-item encoder that maps bag-of-words vectors for combined  reviews  associated  with  each  paired user and item onto structured embeddings, which in turn define per-aspect topic weights. We model individual reviews in a structured manner by infer- ring an aspect assignment for each sentence in a given review, where the per-aspect topic weights obtained by the user-item encoder serve to define a mixture over topics, conditioned on the aspect. The result is an autoencoding neural topic model for reviews, which can be trained in a fully unsupervised manner to learn topics that are structured into aspects.
    </div>
    <div class="pub-links">
      [<a href="http://proceedings.mlr.press/v89/esmaeili19b/esmaeili19b.pdf">PDF</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="seaman_arxiv_2018" src="/assets/images/seaman_arxiv_2018.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Modeling Theory of Mind for Autonomous Agents with Probabilistic Programs
    </div>
    <div class="pub-venue">
      <a href="https://arxiv.org/abs/1812.01569">ArXiv Pre-print</a>;  <a href="https://sites.google.com/view/icml-i3/home">ICML 2019 Workshop on Imitation, Intent, and Interaction</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/iris-rubi-seaman/">Iris Rubi Seaman</a>,
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="https://pcc.cs.byu.edu/testimonial/">David Wingate</a>.
    </div>
    <div class="pub-abst">
    As autonomous agents become more ubiquitous, they will eventually have to reason about the mental state of other agents, including those agents' beliefs, desires and goals - so-called theory of mind reasoning. We introduce a collection of increasingly complex theory of mind models of a "chaser" pursuing a "runner", which are implemented as nested probabilistic programs. We show that planning can be performed using nested importance sampling methods, resulting in rational behaviors from both agents, and show that allocating additional computation to perform nested reasoning about agents result in lower-variance estimates of expected utility.
    </div>
    <div class="pub-links">
      [<a href="https://arxiv.org/abs/1812.01569">arXiv</a>]
      |
      [<a href="https://drive.google.com/drive/folders/1dKC2ptym7eb9PMzxUSU1ee8so46YaZBd">ICML I3 Workshop Abstract</a>]
      |
      [<a href="https://slideslive.com/38917635/nested-reasoning-about-autonomous-agents-using-probabilistic-programs?locale=cs">ICML I3 Workshop Talk</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="sennesh_arxiv_2018" src="/assets/images/sennesh_arxiv_2018.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Composing Modeling and Inference Operations with Probabilistic Program Combinators
    </div>
    <div class="pub-venue">
      <a href="https://arxiv.org/abs/1811.05965">ArXiv Pre-print</a>; <a href="https://sites.google.com/view/nipsbnp2018/accepted-papers">NeurIPS 2018 Bayesian Nonparametrics Workshop</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/eli-sennesh/">Eli Sennesh</a>,
      <a href="https://scholar.google.com/citations?user=Gpw8Z0cAAAAJ">Adam Ścibior</a>,
      <a href="https://www.khoury.northeastern.edu/people/hao-wu/">Hao Wu</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
    We introduce a combinator library for the Probabilistic Torch framework. Combinators are functions that accept models and return transformed models. We assume that models are dynamic, but that model composition is static, in the sense that combinator application takes place prior to evaluating the model on data. Model combinators use classic functional constructs such as map and reduce to define a computation at a coarsened level of representation. Inference combinators alter the evaluation strategy using operations such as importance resampling and application of a transition kernel, whilst preserving proper weighting.
  </div>
    <div class="pub-links">
      [<a href="https://arxiv.org/abs/1811.05965">arXiv</a>]
      | [<a href="https://sites.google.com/view/nipsbnp2018/accepted-papers">BNP@NeurIPS</a>]
    </div>
  </div>
</div>

<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="jain_emnlp_2018" src="/assets/images/jain_emnlp_2018.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Learning Disentangled Representations of Texts with Application to Biomedical Abstracts
    </div>
    <div class="pub-venue">
      <a href="http://emnlp2018.org">Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.khoury.northeastern.edu/people/sarthak-jain-2/">Sarthak Jain</a>,
      <a href="https://www.khoury.northeastern.edu/people/edward-banner/">Edward Banner</a>,
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="https://kclpure.kcl.ac.uk/portal/en/persons/iain-marshall(4ef674b9-9b3b-417b-9449-13d224b9a11c)/publications.html">Iain J. Marshall</a>,
      <a href="http://www.byronwallace.com">Byron C. Wallace</a>,
    </div>
    <div class="pub-abst">
      We propose a method for learning disentangled representations of texts that code for distinct and complementary aspects, with the aim of affording efficient model transfer and interpretability. To induce disentangled embeddings, we propose an adversarial objective based on the (dis)similarity between triplets of documents with respect to specific aspects. Our motivating application is embedding biomedical abstracts describing clinical trials in a manner that disentangles the populations, interventions, and outcomes in a given trial. We show that our method learns representations that encode these clinically salient aspects, and that these can be effectively used to perform aspect-specific retrieval.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/jain_emnlp_2018.pdf">PDF</a>]
    </div>
  </div>
</div>


<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="rainforth_arxiv_2018" src="/assets/images/rainforth_aabi_2017.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Inference Trees: Adaptive Inference with Exploration
    </div>
    <div class="pub-venue">
      <a href="https://arxiv.org/abs/1806.09550/">ArXiv Tech Report</a>
    </div>
    <div class="pub-auth">
      <a href="http://www.robots.ox.ac.uk/~twgr/">Tom Rainforth</a>,
      <a href="https://www.cs.ox.ac.uk/people/yuan.zhou/">Yuan Zhou</a>,
      <a href="https://xiaoyulu2014.github.io">Xiaoyu Lu</a>,
      <a href="https://www.stats.ox.ac.uk/~teh/">Yee Whye Teh</a>,
      <a href="http://www.robots.ox.ac.uk/~fwood/">Frank Wood</a>,
      <a href="https://cs.kaist.ac.kr/people/view?idx=552&kind=faculty&menu=167">Hongseok Yang</a>,
      <a href="/">Jan-Willem van de Meent</a>.
    </div>
    <div class="pub-abst">
      We introduce inference trees (ITs), a new adaptive Monte Carlo inference method building on ideas from Monte Carlo tree search. Unlike most existing methods which are implicitly based on pure exploitation, ITs explicitly aim to balance exploration and exploitation in the inference process, alleviating common pathologies and ensuring consistency. More specifically, ITs use bandit strategies to adaptively sample from hierarchical partitions of the parameter space, while simultaneously learning these partitions in an online manner.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/rainforth_arxiv_2018.pdf">PDF</a>]
    </div>
  </div>
</div>


<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="siddharth_nips_2017" src="/assets/images/siddharth_nips_2017.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Learning Disentangled Representations with Semi-Supervised Deep Generative Models
    </div>
    <div class="pub-venue">
      <a href="http://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models">Neural Information Processing Systems 2017</a>
    </div>
    <div class="pub-auth">
      <a href="http://www.robots.ox.ac.uk/~nsid/">N. Siddharth*</a>,
      <a href="http://www.robots.ox.ac.uk/~brooks/">Brooks Paige*</a>,
      <a href="/">Jan-Willem van de Meent*</a>,
      <a href="http://www.robots.ox.ac.uk/~alban/">Alban Desmaison</a>,
      <a href="https://cocolab.stanford.edu/ndg.html">Noah D. Goodman</a>,
      <a href="https://sites.google.com/site/pushmeet/">Pushmeet Kohli</a>,
      <a href="http://www.robots.ox.ac.uk/~fwood/">Frank Wood</a>,
      <a href="http://www.robots.ox.ac.uk/~phst/">Philip H.S. Torr</a>
    </div>
    <div class="pub-abst">
        We propose to learn disentangled representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/siddharth_nips_2017.pdf">PDF</a>] |
      [<a href="https://github.com/probtorch/probtorch">Code</a>]
    </div>
  </div>
</div>


<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="rainforth_nips_2016" src="/assets/images/rainforth_nips_2016.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Bayesian Optimization for Probabilistic Programs
    </div>
    <div class="pub-venue">
      <a href="https://papers.nips.cc/paper/6421-bayesian-optimization-for-probabilistic-programs">Neural Information Processing Systems 2016</a>
    </div>
    <div class="pub-auth">
      <a href="http://www.robots.ox.ac.uk/~twgr/">Tom Rainforth</a>,
      <a href="http://www.tuananhle.co.uk">Tuan-Anh Le</a>,
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="http://www.robots.ox.ac.uk/~mosb/">Michael A. Osborne</a>,
      <a href="http://www.robots.ox.ac.uk/~fwood/">Frank Wood</a>.
    </div>
    <div class="pub-abst">
      We present the first general purpose framework for marginal maximum a pos- teriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/rainforth_nips_2016.pdf">PDF</a>] |
      [<a href="https://github.com/probprog/bopp">Code</a>]
    </div>
  </div>
</div>


<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="tolpin_fpl_2016" src="/assets/images/tolpin_ifl_2016.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
        Design and Implementation of Probabilistic Programming Language Anglican
    </div>
    <div class="pub-venue">
      <a href="https://doi.org/10.1145/3064899.3064910">Symposium on the Implementation and Application of Functional Programming Languages 2016</a>
    </div>
    <div class="pub-auth">
      <a href="https://www.linkedin.com/in/david-tolpin-1a45736/">David Tolpin</a>,
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="https://cs.kaist.ac.kr/people/view?idx=552&kind=faculty&menu=167">Hongseok Yang</a>,
      <a href="http://www.robots.ox.ac.uk/~fwood/">Frank Wood</a>.
    </div>
    <div class="pub-abst">
      We present the first general purpose framework for marginal maximum a pos- teriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/tolpin_ifl_2016.pdf">PDF</a>] |
      [<a href="https://bitbucket.org/probprog/anglican">Code</a>]
    </div>
  </div>
</div>


<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="vandemeent_aistats_2016" src="/assets/images/vandemeent_aistats_2016.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Black-Box Policy Search with Probabilistic Programs
    </div>
    <div class="pub-venue">
      <a href="http://proceedings.mlr.press/v38/vandemeent15.pdf">Artificial Intelligence and Statistics 2015</a>
    </div>
    <div class="pub-auth">
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="http://www.robots.ox.ac.uk/~brooks/">Brooks Paige</a>,
      <a href="https://www.linkedin.com/in/david-tolpin-1a45736/">David Tolpin</a>,
      <a href="http://www.robots.ox.ac.uk/~fwood/">Frank Wood</a>.
    </div>
    <div class="pub-abst">
      In this work we show how to represent policies as programs: that is, as stochastic simulators with tunable parameters. To learn the parameters of such policies we develop connections between black box variational inference and existing policy search approaches. We then explain how such learning can be implemented in a probabilistic programming system. We demonstrate both conciseness of policy representation and automatic policy parameter learning for a set of canonical reinforcement learning problems.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/vandemeent_aistats_2015.pdf">PDF</a>] |
      [<a href="https://bitbucket.org/probprog/black-box-policy-search">Code</a>]
    </div>
  </div>
</div>


<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="vandemeent_aistats_2015" src="/assets/images/vandemeent_aistats_2015.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Particle Gibbs with Ancestor Sampling for Probabilistic Programs
    </div>
    <div class="pub-venue">
      <a href="http://proceedings.mlr.press/v38/vandemeent15.html">Artificial Intelligence and Statistics 2015</a>
    </div>
    <div class="pub-auth">
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="https://cs.kaist.ac.kr/people/view?idx=552&kind=faculty&menu=167">Hongseok Yang</a>,
      <a href="http://probcomp.org/principal-investigator/">Vikash Mansinghka</a>,
      <a href="http://www.robots.ox.ac.uk/~fwood/">Frank Wood</a>.
    </div>
    <div class="pub-abst">
      Particle Markov chain Monte Carlo techniques rank among current state-of-the-art methods for probabilistic program inference. A drawback of these techniques is that they rely on importance resampling, which results in degenerate particle trajectories and a low effective sample size for variables sampled early in a program. We here develop a for- malism to adapt ancestor resampling, a technique that mitigates particle degeneracy, to the probabilistic programming setting.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/vandemeent_aistats_2015.pdf">PDF</a>]
    </div>
  </div>
</div>


<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="vandemeent_aistats_2014" src="/assets/images/wood_aistats_2014.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      A New Approach to Probabilistic Programming Inference
    </div>
    <div class="pub-venue">
      <a href="http://proceedings.mlr.press/v33/wood14.html">Artificial Intelligence and Statistics 2014</a>
    </div>
    <div class="pub-auth">
      <a href="http://www.robots.ox.ac.uk/~fwood/">Frank Wood</a>,
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="http://probcomp.org/principal-investigator/">Vikash Mansinghka</a>.
    </div>
    <div class="pub-abst">
      We demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. It applies to Turing-complete proba- bilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochas- tic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more e cient than previously introduced single-site Metropolis-Hastings methods.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/wood_aistats_2014.pdf">PDF</a>] | [<a href="https://bitbucket.org/probprog/interpreted-anglican">Code</a>]
    </div>
  </div>
</div>


<div class="pub row">
  <div class="pub-image col-md-3">
    <img alt="vandemeent_bpj_2014" src="/assets/images/vandemeent_bpj_2014_2.jpg">
  </div>
  <div class="pub-info col-md-9">
    <div class="pub-title">
      Empirical Bayes Methods Enable Advanced Population-Level Analyses of Single-Molecule FRET Experiments
    </div>
    <div class="pub-venue">
      <a href="http://dx.doi.org/10.1016/j.bpj.2013.12.055">Biophysical Journal, 2014</a>
    </div>
    <div class="pub-auth">
      <a href="/">Jan-Willem van de Meent</a>,
      <a href="https://www.linkedin.com/in/jonathanbronson/">Jonathan E. Bronson</a>,
      <a href="http://www.columbia.edu/~chw2/">Chris H. Wiggins</a>,
      <a href="http://www.columbia.edu/cu/chemistry/groups/gonzalez/">Ruben L. Gonzalez, Jr.</a>.
    </div>
    <div class="pub-abst">
      We demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. It applies to Turing-complete proba- bilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochas- tic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more e cient than previously introduced single-site Metropolis-Hastings methods.
    </div>
    <div class="pub-links">
      [<a href="/assets/pdf/vandemeent_bpj_2014.pdf">PDF</a>] | [<a href="https://ebfret.github.io">Code</a>]
    </div>
  </div>
</div>

</div>



        </div>
      </div>

      <div class="footer row">
        <p>
    built using 
    <a href="http://jekyllrb.com">jekyll</a>, 
    <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a> and 
    <a href="http://getbootstrap.com">bootstrap</a>. 
</p>        
      </div>
    </div>

    <script src="https://code.jquery.com/jquery.js"></script>

    <script src="/assets/js/bootstrap.min.js"></script>

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-82483238-1']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>

  </body>
</html>
