@article{vandemeent2018introduction,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.10756},
  primaryClass = {cs, stat},
  title = {An {{Introduction}} to {{Probabilistic Programming}}},
  abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
  journal = {arXiv:1809.10756 [cs, stat]},
  author = {{van de Meent}, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  month = sep,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages,Computer Science - Machine Learning},
  file = {/Users/janwillem/Zotero/storage/D3M245LQ/van de Meent - 2018 - An Introduction to Probabilistic Programming.pdf}
}

@article{bateni2020improving,
  title = {Improving {{Few}}-{{Shot Visual Classification}} with {{Unlabelled Examples}}},
  author = {Bateni, Peyman and Barber, Jarred and {van de Meent}, Jan-Willem and Wood, Frank},
  year = {2020},
  month = jun,
  abstract = {We propose a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve new state of the art in-domain performance on Meta-Dataset, and improve accuracy on mini- and tiered-ImageNet as compared to other conditional neural adaptive methods that use the same pre-trained feature extractor.},
  AUTHOR+an = {3=highlight},
  archivePrefix = {arXiv},
  eprint = {2006.12245},
  eprinttype = {arxiv},
  file = {/Users/janwillem/Zotero/storage/JXXWVCPB/Bateni - 2020 - Improving Few-Shot Visual Classification with Unlabelled Examples.pdf},
  journal = {arXiv:2006.12245 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{biza2020learning,
  title = {Learning Discrete State Abstractions with Deep Variational Inference},
  author = {Biza, Ondrej and Platt, Robert and {van de Meent}, Jan-Willem and Wong, Lawson L. S.},
  year = {2020},
  month = mar,
  abstract = {Abstraction is crucial for effective sequential decision making in domains with large state spaces. In this work, we propose a variational information bottleneck method for learning approximate bisimulations, a type of state abstraction. We use a deep neural net encoder to map states onto continuous embeddings. The continuous latent space is then compressed into a discrete representation using an action-conditioned hidden Markov model, which is trained end-to-end with the neural network. Our method is suited for environments with high-dimensional states and learns from a stream of experience collected by an agent acting in a Markov decision process. Through a learned discrete abstract model, we can efficiently plan for unseen goals in a multi-goal Reinforcement Learning setting. We test our method in simplified robotic manipulation domains with image states. We also compare it against previous model-based approaches to finding bisimulations in discrete grid-world-like environments.},
  archivePrefix = {arXiv},
  eprint = {2003.04300},
  eprinttype = {arxiv},
  file = {/Users/janwillem/Zotero/storage/MZIEZZSQ/Biza - 2020 - Learning discrete state abstractions with deep variational inference.pdf},
  journal = {arXiv:2003.04300 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{farnoosh2020deep,
  title = {Deep {{Markov Spatio}}-{{Temporal Factorization}}},
  author = {Farnoosh, Amirreza and Rezaei, Behnaz and Sennesh, Eli Zachary and Khan, Zulqarnain and Dy, Jennifer and Satpute, Ajay and Hutchinson, J. Benjamin and {van de Meent}, Jan-Willem and Ostadabbas, Sarah},
  year = {2020},
  month = mar,
  abstract = {We introduce deep Markov spatio-temporal factorization (DMSTF), a deep generative model for spatio-temporal data. Like other factor analysis methods, DMSTF approximates high-dimensional data by a product between time-dependent weights and spatially dependent factors. These weights and factors are in turn represented in terms of lower-dimensional latent variables that we infer using stochastic variational inference. The innovation in DMSTF is that we parameterize weights in terms of a deep Markovian prior, which is able to characterize nonlinear temporal dynamics. We parameterize the corresponding variational distribution using a bidirectional recurrent network. This results in a flexible family of hierarchical deep generative factor analysis models that can be extended to perform time series clustering, or perform factor analysis in the presence of a control signal. Our experiments, which consider simulated data, fMRI data, and traffic data, demonstrate that DMSTF outperforms related methods in terms of reconstruction accuracy and can perform forecasting in a variety domains with nonlinear temporal transitions.},
  archivePrefix = {arXiv},
  eprint = {2003.09779},
  eprinttype = {arxiv},
  file = {/Users/janwillem/Zotero/storage/4PQ2QMUG/Farnoosh - 2020 - Deep Markov Spatio-Temporal Factorization.pdf},
  journal = {arXiv:2003.09779 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{bozkurt2020rate-regularization,
  title = {Rate-{{Regularization}} and {{Generalization}} in {{VAEs}}},
  author = {Bozkurt, Alican and Esmaeili, Babak and Brooks, Dana H. and Dy, Jennifer G. and {van de Meent}, Jan-Willem},
  year = {2020},
  month = jul,
  abstract = {Variational autoencoders (VAEs) optimize an objective that comprises a reconstruction loss (the distortion) and a KL term (the rate). The rate is an upper bound on the mutual information, which is often interpreted as a regularizer that controls the degree of compression. We here examine whether inclusion of the rate term also improves generalization. We perform rate-distortion analyses in which we control the strength of the rate term, the network capacity, and the difficulty of the generalization problem. Lowering the strength of the rate term paradoxically improves generalization in most settings, and reducing the mutual information typically leads to underfitting. Moreover, we show that generalization performance continues to improve even after the mutual information saturates, indicating that the gap on the bound (i.e. the KL divergence relative to the inference marginal) affects generalization. This suggests that the standard spherical Gaussian prior is not an inductive bias that typically improves generalization, prompting further work to understand what choices of priors improve generalization in VAEs.},
  archivePrefix = {arXiv},
  eprint = {1911.04594},
  eprinttype = {arxiv},
  file = {/Users/janwillem/Zotero/storage/V8VYWG7D/Bozkurt - 2020 - Rate-Regularization and Generalization in VAEs.pdf},
  journal = {arXiv:1911.04594 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}